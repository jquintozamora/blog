{"title":"Jos√© Quinto","description":"I am a software engineer focused on web development and open-source contributor.","language":"en","link":"https://blog.josequinto.com","pubDate":"Sat, 04 Apr 2020 07:00:00 GMT","lastBuildDate":"Tue, 27 Oct 2020 08:56:04 GMT","generator":"hexo-generator-json-feed","webMaster":"Jos√© Quinto","items":[{"title":"Git: sync your fork with the original repo","link":"https://blog.josequinto.com/2020/04/04/git-sync-your-fork-with-original-repo/","description":"IntroductionWhen collaborating with github projects, normally, you do your changes in your fork / branch and then push these changes to your origin ( and when you create a PR it should automatically point to original repo master ). Recently I started collaborating with a github project called excalidraw and they use that approach. Even if that sounds a easy thing to do, it is not, specially new developers could struggle to get sync done for first time. Sync Fork with Original RepoBest and quicker approach to sync your repo is: Let‚Äôs explain the approach: Configure upstream We should have origin: &lt;our fork url&gt; and upstream: &lt;original repo url&gt;. If we don‚Äôt have upstream configured then we should add it. 123456789101112131415$ git remote -vorigin https://github.com/jquintozamora/excalidraw.git (fetch)origin https://github.com/jquintozamora/excalidraw.git (push)$ git remote add upstream https://github.com/excalidraw/excalidraw.git$ git remote -vorigin https://github.com/jquintozamora/excalidraw.git (fetch)origin https://github.com/jquintozamora/excalidraw.git (push)upstream https://github.com/excalidraw/excalidraw.git (fetch)upstream https://github.com/excalidraw/excalidraw.git (push) We should now see both origin and upstream setup properly done. fetch changes from original repo ( upstream ) ‚¨áÔ∏è 12345678$ git fetch upstream -apremote: Enumerating objects: 286, done.remote: Counting objects: 100% (286/286), done.remote: Compressing objects: 100% (66/66), done.remote: Total 322 (delta 249), reused 241 (delta 220), pack-reused 36Receiving objects: 100% (322/322), 125.10 KiB | 1.03 MiB/s, done.... merge changes from upstream into your fork üîÑ 12345$ git merge upstream/masterUpdating 6635261..ae1eee1Fast-forwardchanges... After merge, we will have both repos on sync on our local machine. But, I‚Äôd recommend to push changes to have your fork updated on github too. push to sync changes on github too ‚¨ÜÔ∏è 12345$ git push origin masterTotal 0 (delta 0), reused 0 (delta 0)To https://github.com/jquintozamora/excalidraw.git 6635261..ae1eee1 master -&gt; master üöÄüöÄ DONE üöÄüöÄ I hope it works for you as well, if have any issues, please leave a comment, so we can discuss and try to help!","pubDate":"Sat, 04 Apr 2020 07:00:00 GMT","guid":"https://blog.josequinto.com/2020/04/04/git-sync-your-fork-with-original-repo/","category":"Git"},{"title":"My experience as Oxford Artificial Intelligence Programme 2019 student","link":"https://blog.josequinto.com/2019/07/24/ai-said-business-oxford-university-programme-2019/","description":"One of my professional goals for 2019 was to study something related with Artificial Intelligence (AI). ü§ñ As part of my innovation skills I already have knowledge and expertise designing and implementing voice apps and bots üó£ ( which is another type of AI), but I wanted to learn also Machine Learning and Deep Learning. ü§™ During my search for something to study in an online manner, an advertisement came up with Oxford Artificial intelligence Programme 2019 and that was ideal for me because allowed me study online but also comes from a high recognized University and Business School: Oxford Sa√Ød üîù. So, exciting times, I decided to invest in my skills and enroll into that programme. Programme curriculum Module 1: Artificial intelligence: What is all the hype about? Explore the history and potential of AI within the context of the digital ecosystem. Different AI definitions The timeline of AI State of the Art in AI Applications Module 2: AI‚Äôs engine room: How do machines learn from data? What is Machine Learning? Types of machine learning: supervised, reinforcement, and unsupervised learning. Regression, Classification, Clustering and Decision Types of data and Data Quality Module 3: Current frontiers: Deep learning and the latest trends in AI Understand what deep learning is and how it is powering the modern approach to AI. Artificial Neural Networks Forward Propagation Backpropagation Applications of Neural Networks and deep learning Success cases and examples Module 4: Working with intelligent machines Explore the concept of intelligence in machines Different forms of Intelligence The impact of AI in the labour market. Module 5: Setting the boundaries of AI Explore the hierarchy of ethical and legal considerations around AI. Boundaries of AI Law and AI Ethics and AI Module 6: Making a business case for AI Identify the potential business opportunities of AI in a specific context. AI Business Cases The approach and graduation processIn my opinion, one of the challenges for success on the programme is the fact that most of students are working and studying at the same time, so the combination of flexibility and also achievable deadlines is important. Also a good structure in terms of independent units for every module. üëç In terms of approach, I liked the approach of having graded activities at the end of each module. Also, I liked having a forum platform in which all students discuss further any topics and share different experiences in different sectors. In terms of professors, I liked the professors were actively involved in our discussions and gave proactive feedback after every module. I‚Äôd definitely recommend that / or similar online programmes. Final GradesI completed the program in July 2019 üë®üèª‚Äçüíª with a score of 81 out of 100 ü§ì, I am proud of it due to the high average level in the class. üöÄ Next StepsAfter that course of AI and ML, I do have a better overall on Machine Learning and its applications in real life. My next steps towards AI skills are potentially: advanced Machine Learning course and side projects to grown my ML skills. üí™üèªü§ñ","pubDate":"Wed, 24 Jul 2019 17:00:24 GMT","guid":"https://blog.josequinto.com/2019/07/24/ai-said-business-oxford-university-programme-2019/","category":"AI,Oxford University"},{"title":"Alexa, certify me on Skill Building","link":"https://blog.josequinto.com/2019/04/18/aws-certified-alexa-skill-builder/","description":"Back in January 2019, Amazon launched beta exams for AWS Certification Alexa Skill Builder - Specialty. Then I signed up to keep up my Alexa Skills knowledge. Previously, I worked designing and implementing the Eurostar Skill for Alexa during 6 months as par of the Innovation Team at Eurostar. It was quite intense project and at the same time exciting to work in that new technology in collaboration with Amazon Team in London. I learnt a lot, so I was looking forward to put all that learning into a certification. Today, I‚Äôve received an email from Amazon with the good news! I‚Äôm happy to announce I‚Äôve become AWS Alexa Skill Builder Certified! Download Certification","pubDate":"Thu, 18 Apr 2019 17:00:24 GMT","guid":"https://blog.josequinto.com/2019/04/18/aws-certified-alexa-skill-builder/","category":"Certification"},{"title":"Wrapping up 2018","link":"https://blog.josequinto.com/2019/02/24/wrapping-up-2018/","description":"I started 2018 working for Arcadia Group in London as a React Developer, there I met lot of smart colleagues and friends working hard on the isomorphic React Application within the Catalogue Team. We were working following strictly scrum methodology (Planning, Refinement, Standups, 3 Amigos, Retrospective). In terms of tech stack, I learned a lot, while I created new React reusable components, redux actions and reducers. Also, I spent lof of time creating Unit Test with Jest and Enzyme (redux-mock-store, snapshots, jest.mock, jest.fn, jest.spyOn, and so on). Eventually, after 8 months, I left Arcadia Group to begin another kind of professional trip, becoming contractor in London! Then I closed my eyes and make myself comfortable with different kind of interviews and recruiter agents. It was harder than I expected but eventually I got my first contract as a Senior JavaScript Full Stack Software Engineer at Eurostar. I appreciate the opportunity I was given here starting my new role as in the Innovation Team. The methodology was quite different as per the nature of the team we don‚Äôt follow strict Agile Scrum, instead, we do follow Kanban approach ( weekly planning, standups, demo fridays, jira board, and so on). In terms of tech stack, I started solving some bugs for some of the microservices using JavaScript and NodeJS (AWS, SQS, SNS, Queue Consumer) and I learnt new things like AWS ( I used Azure until then ), CircleCI, Terraform, Microservices Architecture. After few months I then joined to a new challenge to build the Alexa Eurostar Skill. That started as a POC being a conversational ad-hoc script which I prepared in a week for a show and tell presentation. Initially, I was quite confused about the scope for that POC but after the good result of the POC, Eurostar decided to build a Team around that skill and then we end up being a team of 2 developers, 1 voice ux, 1 product owner and 1 scrum manager, I really liked the new shape for that, and even if I was quite sceptical at the begining, eventually, I started believing in voice systems!! Crazy and exciting days, isn‚Äôt it? üöÄ We started to take that project really seriously and start doing lots of research in order to become one of the first doing things right with Alexa Skill Development. I event took the time to write down several posts with our investigations and results. From our Alexa Skill project we started small different sub-projects following the proper microservices infrastructure we have at Eurostar. I can mention some of them: Alexa Skill: That is a project intended to design, develop, continuously integrate and deploy our skill in Lambda function and Alexa Development Console (ASK). Among others we used: ASK CLI AWS SDK AWS CLI NodeJS Jest Intent Debugger ( custom ) Github / Alexa Dev Console / AWS Lambda integration and synch scripts date-fns axios Apollo Client Fallback Intent Yes / No Custom Slot Custom Slots for Stations SearchQuery Slot Type Voice BFF: We built a backend for frontend layer in order to communicate with our Internal and External APIs. Among others we used that stack: NodeJS Express GraphQL Apollo Server 2 Babel Docker ( for Integration Tests ) mockttp ( for creating stubs for external APIs ) Jest Voice NER: We build a project for improving the Name Entity Recognition currently built-in within Alexa, we had different issues with the users being able to make Alexa recognizing certain expressions like ‚Äòin two weeks time‚Äô, ‚Äònext month for two days‚Äô, and so on. And we decided to create a NER to actually parse the text given by Alexa and extract from there the right dates we needed. We explored and used a combination of different date parses and NERs on the internet: Stratford University NER Microsoft NER Library ( npm ) WatsonJS Date Parser ( npm ) Our custom parser You can imagine such a six months we spent investigating, learning and developing all that stuff and finally releasing the [Alexa Eurostar Skill] (https://www.amazon.co.uk/Eurostar/dp/B07K8RVXF7). Which is not only Certified and Publish by Amazon in the Skill Market Store, but also being tested by real users in a bunch of user testing sessions our Voice UX did which such a passion and energy. It has been a success story in and outside Eurostar and we are going to speak in the next Alexa Developer Meetup in London ( Amazon UK Offices ) and tell the world how we did and our experiences at different levels ( dev, ux and lessons learn ). I also achieved the AWS Certified Alexa Skill Builder title after 3 long hours of beta exam. Now, 2019 is going to be a year of fresh project and new challenges. I will be starting in the BPA Team at Eurostar working on the Booking and Checkout area in the eurostar.com website. Exiting times! I‚Äôm really looking forward starting that new project in January and doing lots of things in React, Redux and NodeJS. Also, after my project developing Amazon Alexa Skill, I do believe 2019 is going to be a great year for voice systems and I will start some side projects creating my own Alexa Skill on my free time!","pubDate":"Sun, 24 Feb 2019 18:00:24 GMT","guid":"https://blog.josequinto.com/2019/02/24/wrapping-up-2018/","category":"Wrapping up"},{"title":"Configuring permissions to request email access for use in your Alexa Skill","link":"https://blog.josequinto.com/2018/08/06/configuring-permissions-to-request-email-access-for-use-in-your-skill-alexa/","description":"IntroductionWhen we are designing and implementing a new Alexa Custom Skill, one of the first questions we ask ourselves is whether we have access to user‚Äôs email or not. Recently (July 2018), Amazon Alexa‚Äôs Team released a new feature to allow Alexa Skill‚Äôs Developers to request users to access resources like email, phone and customer name. Until that time, we only were able to request permissions for Device Address and Lists. There is a blob post from Alexa&#39;s Team explaining how to request customer contact information. Account Linking vs Permissions on gathering user‚Äôs emailUntil now, there were some scenarios like sending detailed information to customers over email that required Account Linking either with Amazon or Custom Authentication Provider. But with this new feature if we only do Account Linking to gather the user email, then we don‚Äôt need it anymore and we can use this simpler approach to get permissions from the user and get their email. Skill configuration to request customer permissions.Using Amazon Developer Console we can manage our skill. Navigate to the Build -&gt; Permissions page in the console and select Customer Email Address: Note: If you are using ask-cli to configure and update your skill then you should add permissions in your skill.json: Get user‚Äôs email in our codeIn order to get the email to be used in our code, we have to do query Amazon‚Äôs API asking for it and we have to use the field apiAccessToken provided under context -&gt; System in our handlerInput object. apiAccessTokenBefore going forward and showing the code, I‚Äôd like to share the different scenarios we can have and the different outputs for the property apiAccessToken. User denies Email Address permission 12345678910111213141516\"context\": &#123; \"System\": &#123; \"application\": &#123; \"applicationId\": \"amzn1.ask.skill.&lt;skill-id&gt;\" &#125;, \"user\": &#123; \"userId\": \"amzn1.ask.account.&lt;user-id&gt;\" &#125;, \"device\": &#123; \"deviceId\": \"amzn1.ask.device.&lt;device-id&gt;\", \"supportedInterfaces\": &#123;&#125; &#125;, \"apiEndpoint\": \"https://api.amazonalexa.com\", \"apiAccessToken\": \"eyJ0e....&lt;rest-of-jwt-token&gt;\" &#125;&#125; Note that even if we have no permissions added to our skill, we still get apiAccessToken property. If we inspect apiAccessToken JWT token using https://jwt.io, we can see the below payload data where privateClaims.consentToken is null: 12345678910111213&#123; \"aud\": \"https://api.amazonalexa.com\", \"iss\": \"AlexaSkillKit\", \"sub\": \"amzn1.ask.skill.&lt;skill-id&gt;\", \"exp\": 1633532432, \"iat\": 1633528832, \"nbf\": 1633528832, \"privateClaims\": &#123; \"consentToken\": null, \"deviceId\": \"amzn1.ask.device.&lt;device-id&gt;\", \"userId\": \"amzn1.ask.account.&lt;user-id&gt;\" &#125;&#125; User grants Full Name permission but denies Email Address permission. 12345678910111213141516171819\"context\": &#123; \"System\": &#123; \"application\": &#123; \"applicationId\": \"amzn1.ask.skill.&lt;skill-id&gt;\" &#125;, \"user\": &#123; \"userId\": \"amzn1.ask.account.&lt;user-id&gt;\", \"permissions\": &#123; \"consentToken\": \"eyJ0e...&lt;rest-of-jwt-token&gt;...lFkHDw\" &#125; &#125;, \"device\": &#123; \"deviceId\": \"amzn1.ask.device.&lt;device-id&gt;\", \"supportedInterfaces\": &#123;&#125; &#125;, \"apiEndpoint\": \"https://api.amazonalexa.com\", \"apiAccessToken\": \"eyJ0e...&lt;rest-of-jwt-token&gt;...rTyOD\" &#125;&#125; Note, apart from having a different apiAccessToken, now we also have a new permissions property under user. Now, if we inspect apiAccessToken using https://jwt.io, we can see how consentToken is not null because it has permissions for Name included on it. But still if we try to authenticate to Amazon‚Äôs API to query for email with that token, it will fail with 403 Forbidden: 12345678910111213&#123; \"aud\": \"https://api.amazonalexa.com\", \"iss\": \"AlexaSkillKit\", \"sub\": \"amzn1.ask.skill.&lt;skill-id&gt;\", \"exp\": 1633532432, \"iat\": 1633528832, \"nbf\": 1633528832, \"privateClaims\": &#123; \"consentToken\": \"Atza|...&lt;rest-of-amazon-token-authorizing-full-name&gt;...\", \"deviceId\": \"amzn1.ask.device.&lt;device-id&gt;\", \"userId\": \"amzn1.ask.account.&lt;user-id&gt;\" &#125;&#125; User grants Email Address permission 12345678910111213141516171819\"context\": &#123; \"System\": &#123; \"application\": &#123; \"applicationId\": \"amzn1.ask.skill.&lt;skill-id&gt;\" &#125;, \"user\": &#123; \"userId\": \"amzn1.ask.account.&lt;user-id&gt;\", \"permissions\": &#123; \"consentToken\": \"eyJ0e...&lt;rest-of-jwt-token&gt;...lFkHDw\" &#125; &#125;, \"device\": &#123; \"deviceId\": \"amzn1.ask.device.&lt;device-id&gt;\", \"supportedInterfaces\": &#123;&#125; &#125;, \"apiEndpoint\": \"https://api.amazonalexa.com\", \"apiAccessToken\": \"eyJ0e...&lt;rest-of-jwt-token&gt;...rTyOD\" &#125;&#125; It is almost look a like of scenario‚Äôs 2 token, the difference is consentToken is different and now it does allow querying for email. 12345678910111213&#123; \"aud\": \"https://api.amazonalexa.com\", \"iss\": \"AlexaSkillKit\", \"sub\": \"amzn1.ask.skill.&lt;skill-id&gt;\", \"exp\": 1633532432, \"iat\": 1633528832, \"nbf\": 1633528832, \"privateClaims\": &#123; \"consentToken\": \"Atza|...&lt;rest-of-amazon-token-authorizing-email&gt;...\", \"deviceId\": \"amzn1.ask.device.&lt;device-id&gt;\", \"userId\": \"amzn1.ask.account.&lt;user-id&gt;\" &#125;&#125; User grants Email Address permission + User authenticate through Account Linking as well We can have both working together, Account Linking + Permissions for email. The main difference in terms of the handlerInput schema is, we will user.accessToken property which we can use to authenticate and authorize against our custom Authentication Server: 1234567891011121314151617181920\"context\": &#123; \"System\": &#123; \"application\": &#123; \"applicationId\": \"amzn1.ask.skill.&lt;skill-id&gt;\" &#125;, \"user\": &#123; \"userId\": \"amzn1.ask.account.&lt;user-id&gt;\", \"accessToken\": \"eyJ0...&lt;jwt-account-linking-access-token&gt;...\", \"permissions\": &#123; \"consentToken\": \"eyJ0e...&lt;rest-of-jwt-token&gt;...lFkHDw\" &#125; &#125;, \"device\": &#123; \"deviceId\": \"amzn1.ask.device.&lt;device-id&gt;\", \"supportedInterfaces\": &#123;&#125; &#125;, \"apiEndpoint\": \"https://api.amazonalexa.com\", \"apiAccessToken\": \"eyJ0e...&lt;rest-of-jwt-token&gt;...rTyOD\" &#125;&#125; Note that giving permissions and doing account linking are two separate processes for Alexa Skill. So, they can be used together or by its own. Calling Amazon‚Äôs API to get user‚Äôs emailFinally, after understanding what apiAccessToken does for the different scenarios, let‚Äôs put our hands on and actually do the code that will access to Amazon‚Äôs API from our intent: 123456789101112131415161718192021222324252627282930313233343536373839404142434445const axios = require(\"axios\");module.exports = &#123; canHandle(handlerInput) &#123; const request = handlerInput.requestEnvelope.request; return ( request.type === \"IntentRequest\" &amp;&amp; request.intent.name === \"MyIntent\" ); &#125;, async handle(handlerInput) &#123; const &#123; apiAccessToken, apiEndpoint, user &#125; = handlerInput.requestEnvelope.context.System; console.log(\"apiAccessToken: \", apiAccessToken); console.log(\"apiEndpoint: \", apiEndpoint); console.log(\"userId: \", user.userId); const getEmailUrl = apiEndpoint.concat( `/v2/accounts/~current/settings/Profile.email` ); console.log(\"getEmailUrl\", getEmailUrl); let result = \"\"; try &#123; result = await axios.get(getEmailUrl, &#123; headers: &#123; Accept: \"application/json\", Authorization: \"Bearer \" + apiAccessToken &#125; &#125;); &#125; catch (error) &#123; console.log(error); &#125; const email = result &amp;&amp; result.data; return handlerInput.responseBuilder .speak(\"Your email is: \" + email) .getResponse(); &#125;&#125; Recap We can use either Amazon Development Console or ASK CLI to request access for Email permissions to our users. Account Linking and Permissions are independent features and they can be used together or separately. Depending on the configuration and the user input we will have different outputs on apiAccessToken. Accessing to user‚Äôs email will require to call Amazon‚Äôs API with apiAccessToken as a Authentication Bearer header.","pubDate":"Mon, 06 Aug 2018 07:00:24 GMT","guid":"https://blog.josequinto.com/2018/08/06/configuring-permissions-to-request-email-access-for-use-in-your-skill-alexa/","category":"How-To"},{"title":"How to architecture and implement Integration Tests for GraphQL BFF API using docker","link":"https://blog.josequinto.com/2018/07/18/integration-tests-bff-graphql/","description":"IntroductionI‚Äôve been tasked with an interesting one this time. I‚Äôve to create a BFF (Backend For Frontends) API. In this post, I‚Äôll go through the BFF concept, BFF technical stack and also I‚Äôll explain in detail how to create integration tests for that BFF API. BFF (Backend for Frontends)In a nutshell, BFF is built to allow client apps to have more friendly, flexible and manageable API to act as a proxy for the backend services. In a microservices architecture that‚Äôs quite important as it is also used as a scoped schema for different devices or platforms. For example, we could create a BFF API for Web, Mobile and Voice because each of these need different data and combinations of queries from the microservices. For example, an App could need interaction with different type of connectors (database, microservices, REST endpoints, external APIs, and so on) 1234567App | |-&gt; BFF Service | |-&gt; Custom Database |-&gt; User Account Microservice |-&gt; External Weather API Technical StackA BFF service can be done in hundred different ways, in my case I chose: NodeJS babel as ES6 compiler express as a webserver morgan and winston as a logger tools helmet as a Security middleware for express graphql as SDL (Schema Definition Language) apollo-server-express as a GraphQL Server middleware for express nodemon as a dev server reload tool husky and lint-staged as a git hook tool eslint as a linter tool jest as a testing framework circleci as CI tool Integration TestsIntegration Tests are really important for the QA of an API. Also, when working on microservices architecture, it is is even more necessary. Normally, when developing big applications it turns out we have 50 different microservices connected between them. As a Integration Test developers we have to make sure we only test our scope of the app (which is our current service). In a nutshell, we have inputs and outputs in our BFF layer as well: Inputs ‚Äì&gt; Service ‚Äì&gt; Outputs Let‚Äôs ask some questions I had when started building the Integration Testing Workflow: How can we test our API in a isolated way? Using docker, we can create isolated environment for our API under test and also for the test framework itself. How can we test our API using production-like environment? Using docker, we can create a container with the same configuration as production using NODE_ENV=production. How can we provide an easy and automated way to run our integration tests? Using docker-compose, we can easily create a definition for our docker containers and run our integration tests quickly. How can we improve the manageability and speed on developing integration tests? Using jest snapshots to store the expected result (we save some time writing stubs here). So, we choose jest as a testing framework. How can we mock HTTP responses for the external services that our API calls? Using mockttp we can easily mock external services within our tests. When mocking external services HTTP responses, should we mock changing the behavior on the code or should we intercept HTTP responses from the external APIs? Intercept HTTP responses with mockttp. Then we run the integration tests in a more similar to production environment. That means we test the whole workflow: graphql server, resolvers and data sources. Are we going to use Cucumber and Gherkin? Gherkin is high level language used on BDD (Behavior Driven Development) helping other stakeholders to define requirements. That is more used for end to end tests than integration tests. How many integration tests should we write when testing GraphQL Server? I‚Äôm still working on the best approach here, but some ideas: Create one file for each query defined on Query schema. Write tests for queries on all levels of the GraphQL Schema. Write tests for error queries which will result on a GraphQL Schema error. How can we mock the behavior of a Custom Database? If you require database queries, use docker-compose to run your local database with seeds. Integration Test Stack NodeJS docker as container service docker-compose as a admin tool for docker mockttp as HTTP mock server and proxy jest as testing framework (snapshots included) apollo-boost as apollo client Requirements NodeJS version ^8.11.3 docker docker-compose schema version 3.6 ArchitectureIntegration Tests are going to run in docker containers in order to have similar to production environment in which to run tests. 12345678my-bff-service (container) - API under test - Listening on 5555integration-tester (container) - Call voice-bff API - Use `mockttp` to mock external services calls - Listening on 8888 Structure12345678910111213141516171819202122my-bff-service‚îú‚îÄ‚îÄ test‚îÇ ‚îî‚îÄ‚îÄ integration # Integration Test folder for BFF‚îÇ ‚îú‚îÄ‚îÄ endpoints # Tests folder for endpoints (GraphQL methods)‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ __snapshots__ # jest snapshots folder (API output snapshots)‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ &lt;name&gt;.test.js.snap # One snapshot file for each test file‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ &lt;name&gt;.test.js # Test files written in JS (jest)‚îÇ ‚îú‚îÄ‚îÄ queries # GraphQL queries to import in our tests‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ &lt;method&gt;.&lt;param&gt;.gql # Name queries files with name of the query + params‚îÇ ‚îú‚îÄ‚îÄ stubs # Stubs for external services called in our API under test (mockttp)‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ &lt;external-API-Name&gt; # Create a folder for each API to mock‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ &lt;path&gt;.json # Create a mock for each path or different query‚îÇ ‚îú‚îÄ‚îÄ .babelrc # jest is using babel behind scenes‚îÇ ‚îú‚îÄ‚îÄ .dockerignore # files to be ignored by docker‚îÇ ‚îú‚îÄ‚îÄ docker-compose.yml # docker-compose file where containers will be defined‚îÇ ‚îú‚îÄ‚îÄ Dockerfile # Dockerfile definition for integration-tester container‚îÇ ‚îú‚îÄ‚îÄ integration.env # environment variables used for integration testing‚îÇ ‚îú‚îÄ‚îÄ package.json # integration-tester is a separate project to run in docker ‚îÇ ‚îî‚îÄ‚îÄ test.sh # script to automate docker containers creation and execution‚îú‚îÄ‚îÄ .dockerignore # files to be ignored by docker ‚îú‚îÄ‚îÄ Dockerfile # Dockerfile definition for my-bff-service container‚îî‚îÄ‚îÄ src # BFF Application (GraphQL Server) Running Tests: Development ModeOn development mode, we will need watch mode for test files and API under test. That means is not ideal to develop tests using the docker workflow because it would be so slow. We are going to use our local development server configured for integration tests + running our tests locally. After developing the tests, then we will test using the containers architecture. There are two steps to run integration tests in development mode: Start BFF on Integration Test mode12cd &lt;root of the project&gt;npm run dev:integration-tester Note: Running using dev config is going to restart the server is we do some changes. Run Integration Tests locally123cd ./test/integrationnpm installnpm run test -- --watch Note: Running jest on watch mode is going to restart the test result each time we save changes. Running Tests: Production-like ModeThat‚Äôs the moment of the truth. We will be running our tests inside of a docker container against our API under test which also is going to be inside of other docker container. Then we have isolated environments. 12cd &lt;root of the project&gt;sudo npm run test:integration OR 12cd ./test/integrationsudo ./test.sh Note: Run commands with sudo because docker-compose requires it. Docker help Build and Run docker-compose definition 1sudo docker-compose up --build Stop and Remove docker-compose containers, images, networks, ‚Ä¶ 1sudo docker-compose down --rmi &apos;local&apos; Remove non-used docker images 1sudo docker image prune -fa See all docker images 1sudo docker images -a TODO Use snapstub to automate the creation of stubs for external services mocks","pubDate":"Wed, 18 Jul 2018 17:00:24 GMT","guid":"https://blog.josequinto.com/2018/07/18/integration-tests-bff-graphql/","category":"Investigation,Workflow Performance"},{"title":"Configuring linter, git hooks and auto-format to improve our development workflow","link":"https://blog.josequinto.com/2018/06/14/configuring-linter-git-hooks-improve-development-workflow/","description":"IntroductionThis post is about configuring a linter, git hooks and auto-format on VS Code in order to improve our development workflow. This configuration can be used for any project, but in that particular case I‚Äôll add specific linting rules that applly to our Alexa Skill‚Äôs code. In order to cover all the linting options and functionality we want to configure the linter on different points of our workflow: Create npm run lint to be run linter on demand. Create git hook precommit to run our linter using husky and lint-staged Create git hook prepush to run our test framework using husky LinterThere are different methods and options to consider when we want to add linting to our projects: Prettier: that is a good option for automatically format our code (but at the same time is quite risky as it will decide for you eventually) Eslint: that is really configurable by using rule sets, and it also integrates really well with VS Code auto-format via eslint extension for VS Code. I‚Äôm going to use eslint for that project Add linter, git hooks and rules to our projectAlexa Skill could be written using NodeJS and the Standard JavaScript linting rules. I‚Äôll show how to configure our project to add all the requirements mentioned above. package.json1234567891011121314151617181920212223242526272829&#123; \"name\": \"alexa_skill\", \"version\": \"1.0.0\", \"description\": \"Alexa Skill\", \"main\": \"index.js\", \"scripts\": &#123; \"deploy\": \"ask deploy\", \"test\": \"jest\", \"lint\": \"eslint .\", \"precommit\": \"lint-staged\", \"prepush\": \"npm run test\" &#125;, \"author\": \"Jose Quinto - https://blog.josequinto.com\", \"devDependencies\": &#123; \"ask-cli\": \"1.2.0\", \"eslint\": \"4.19.1\", \"eslint-config-standard\": \"11.0.0\", \"eslint-plugin-import\": \"2.12.0\", \"eslint-plugin-node\": \"6.0.1\", \"eslint-plugin-promise\": \"3.8.0\", \"eslint-plugin-standard\": \"3.1.0\", \"husky\": \"0.14.3\", \"jest\": \"23.1.0\", \"lint-staged\": \"7.1.3\" &#125;, \"lint-staged\": &#123; \"*.js\": [\"npm run lint\", \"git add\"] &#125;&#125; Add devDependencies: eslint, husky, lint-staged, eslint-config-standard and eslint-plugins Run this command: npm i -D eslint husky lint-staged eslint-config-standard eslint-plugin-import eslint-plugin-node eslint-plugin-promise eslint-plugin-standard Add scripts: test, lint, precommit and prepush 1234\"test\": \"jest\",\"lint\": \"eslint .\",\"precommit\": \"lint-staged\",\"prepush\": \"npm run test\" Add lint-staged section. Apply to all *.js files that have been committed and run firstly npm run lint and then git add. Then before doing the git commit the linter is going to run. If the linter fails, then git commit will not be done. .eslintrcAs part of eslint configuration we should add .eslintrc file. For the specific scenario of Alexa Skill, that is the set of rules that better works for our team: 1234567891011121314151617181920212223242526272829303132333435363738&#123; \"root\": true, \"extends\": \"standard\", \"parserOptions\": &#123; \"sourceType\": \"module\" &#125;, \"rules\": &#123; \"indent\": [2, 2, &#123; \"SwitchCase\": 1 &#125;], \"linebreak-style\": [2, \"unix\"], \"semi\": [2, \"always\"], \"no-console\": [2, &#123; \"allow\": [\"warn\", \"error\"] &#125;], \"space-before-function-paren\": 0, \"arrow-body-style\": 0, \"class-methods-use-this\": 0, \"comma-dangle\": [2, \"never\"], \"consistent-return\": 0, \"func-names\": 2, \"global-require\": 0, \"max-len\": 0, \"no-confusing-arrow\": 0, \"no-mixed-operators\": 0, \"no-nested-ternary\": 0, \"no-param-reassign\": 0, \"no-plusplus\": 0, \"no-shadow\": 0, \"no-underscore-dangle\": 0, \"quotes\": [ 2, \"double\", &#123; \"avoidEscape\": true, \"allowTemplateLiterals\": true &#125; ] &#125;, \"env\": &#123; \"node\": true, \"es6\": true, \"jest\": true &#125;&#125; .eslintignoreIn order to make sure we are not linting external files and libraries we should ignore node_modules folder using .eslintignore file: 1234!.*# node_modules ignored by default**/node_modules/* VS ExtensionsIf we are using VS Code, we also can suggest or recommend some extensions which will help providing ‚Äúlive lintint‚Äù on the files opened by VS Code. We can add under .vscode folder, a file called extensions.json with that object on it: 123&#123; \"recommendations\": [\"dbaeumer.vscode-eslint\"]&#125; Then, when a new developer opens that project, will be suggested to install eslint extension. Note: Eslint VS Code extension will also use our .eslintrc rules when we use auto-formating on our files, applying as many rules as it can. VS Code Auto-Formatting shortcuts: MacOS: ‚áß + ‚å• + F Windows: Shift + Alt + F Linux: Control + Shift + I ConclusionThis configuration is not only for Alexa Skill project, we use it in some of our JS, React and NodeJS projects and it really helps on improving our development workflow performance. Having that configuration in our project will help on: Running linter on demand -&gt; npm run lint Running linter before commit -&gt; git commit Running tests before push -&gt; git push Running linter on each file in live (using VS Code Extension) and being able to solve some issues with auto-formatting.","pubDate":"Thu, 14 Jun 2018 07:00:24 GMT","guid":"https://blog.josequinto.com/2018/06/14/configuring-linter-git-hooks-improve-development-workflow/","category":"Code-Reminder,Workflow Performance"},{"title":"Creating a Development Pipeline for Alexa Skill on a multideveloper environment","link":"https://blog.josequinto.com/2018/06/11/development-pipeline-alexa-skill-multideveloper-environment/","description":"IntroductionThere are several challenges in developing Alexa Skills in a multideveloper environment because the way in which is tested and deployed the skill itself. I‚Äôm proud for being part of a team which are going to start building an Alexa Skill. That‚Äôs exciting project that involves thinking in a different way at all levels, from thinking about Intents instead of functions or modules until configuring the development environment pipeline to keep on synch a multideveloper environment. ¬°¬°üöÄüí™ Challenge accepted üí™üöÄ!! First Steps (Team, Requirements and Design)First of all, our current team consists on Product Owner üë®‚ÄçüöÄ, Voice Apps UX üë®‚Äçüé®, Voice Apps QA üë®‚Äçüî¨, and Developersüë®‚Äçüíª. I was never involved on the requirements gathering and design stages for a Voice App, but the experience so far is quite different given the fact people is going to use our skill using voice at home and normally in a relaxed environment with no devices on hands. Development PipelineOne of the ‚ö°Ô∏è main challenges ‚ö°Ô∏è in Alexa Skill‚Äôs multideveloper environment is to keep in synch all the development environments without override others changes. The reason why that is quite difficult is because, nowadays, is not possible to develop / test our skill‚Äôs intents locally. So we need to sign in into Alexa Skill Kit Developer Account in order to be able to see and test our intents and interaction models. Then, as a developer I do need to make changes and / or deploy changes for interaction model on Alexa Cloud and use our local environment to develop the intent‚Äôs code. In a nutshell, our Alexa Skill will need to store files in two places to be ready for test and try: Skill Metadata file and Interaction Model Definition file are going to be published into one Alexa Skill Kit Developer Console account (Amazon‚Äôs Cloud). Lambda Function Code is going to be published in Amazon Web Services&#39; Lambda function There is no problem on having to deploy each part of the Alexa Skill on different cloud places, as we can see on that post, we can use some cli tools to help with the automatization of it. But the problem is that each developer needs to have the model and code deployed in order to test it and we don‚Äôt want override each other continuously. Then, after few brainstorming meetings we decided: Each developer is going to have one different Alexa Skill Kit Development Console account. And also, is going to create its own skill on it. Each developer is going to have one different AWS account also linked with the Amazon Developer account. For each developer we are going to create a lambda function with that name: ask-&lt;skillType&gt;-&lt;skillName&gt;-&lt;profile&gt;, where profile is the name of the profile created for that user. There will be an Amazon Developer account for Staging (profile =&gt; STG) There will be an AWS Lambda Function for Staging (profile =&gt; STG). There will be an Amazon Developer account for PROD (profile =&gt; PROD). The production Amazon Developer account will be the only one that eventually is going to Deploy and Publish the Skill to Amazon SKill‚Äôs Store. There will be an AWS Lambda Function for Production (profile =&gt; PROD). We are going to use Github as a Control Version System and single source for the truth. Multideveloper Environment SyncIn order to be able to sync Alexa Skill environment we have to build some scripts to push changes to cloud from local and the other way around. Pull changes from Github and push them into Dev Environment (Getting the Dev environment ready with the latest changes)‚òëÔ∏è 1. Dev will pull from github for latest changes ‚òëÔ∏è 2. Script to backup Interaction Model before pushing the new one ‚òëÔ∏è 3. Dev will deploy changes related with Interaction Model and Skill Metadata using ask cli tool ‚òëÔ∏è 4. Script to backup the Lambda function code before pushing the new one ‚òëÔ∏è 5. Dev will deploy changes related with Lambda Function Code to AWS using aws cli tool Pull changes from Dev Environment and push them into Github‚òëÔ∏è 1. Script to backup our local Interaction Model before overriding it ‚òëÔ∏è 2. Dev will get the model and skill metadata from Alexa Developer Console using ask cli tool ‚òëÔ∏è 3. Script to backup our local Lambda Function Code before overriding it ‚òëÔ∏è 4. Dev will get lambda function code from AWS using ask cli tool ‚òëÔ∏è 4.1. Dev will implement some changes (optional) ‚òëÔ∏è 5. Dev will push into Github using a feature branch TestingIn terms of testing we will create a hook up to git commit in order to run linter and test before pushing changes and also we will integrate with Circle CI and Github. Unit Tests will be created at Intent level. Integration Tests will be created if there are some external services and / or databases for the skill. Multiple Flow Tests will be created for each Intent. Happy Voice App Coding üé§üé§üé§!","pubDate":"Mon, 11 Jun 2018 07:00:24 GMT","guid":"https://blog.josequinto.com/2018/06/11/development-pipeline-alexa-skill-multideveloper-environment/","category":"How-To"},{"title":"Deploying Alexa Skill using an already created lambda function and role","link":"https://blog.josequinto.com/2018/04/25/deploy-alexa-skill-using-already-created-lambda-function-and-role/","description":"IntroductionWhen developing a custom Alexa skill, if your role is developer üíª and also devops ‚öôÔ∏è for your Alexa Skill, then you‚Äôll have full permissions. So, in order to automatize the deployment of your custom Alexa Skill, you‚Äôll probably end up using ask-cli client tool, specifically the command ask deploy which creates a new AWS Lambda function named ask-&lt;skillType&gt;-&lt;skillName&gt;-&lt;profile&gt; (with the appropriate type, name, and profile for &lt;skillType&gt;, &lt;skillName&gt;, profile). The AWS Lambda function is created with an IAM role named ask-lambda-&lt;skill name&gt;-&lt;profile&gt;, attached to the basic execution policy. However, if you are a developer üíª and you rely on devops ‚öôÔ∏è to create your AWS resources, you‚Äôll probably ask them to create new lambda function and role to start working with our custom Alexa Skill. And probably, the name of the lambda role and function are different than what ask-cli is using and expecting by default. This post is to explain in detail how to automatically deploy a custom Alexa Skill which uses an already created lambda function and role with a different names than the default ones. Create and Deploy Custom Alexa Skill Before continue, If not familiar with custom Alexa Skill, I recommend to quick read an article I wrote about main concepts, requirements, project structure and code samples for custom Alexa Skills. I‚Äôm going to use City Guide official code sample as a baseline or boilerplate for this post. In that code sample, there is a complete guide about how to create and deploy a custom skill but not in a automatic way. We are going to show how to automatically deploy using ask-cli. Create new Skill using code sample Double check you have installed ASK CLI 1ask Initialize credentials profile We have to configure credentials to access Alexa Skills Kit Developer Console and AWS Console. 1ask init Select Create a new profile and enter a name Select default as a AWS Profile to deploy Lambda function, then you will be redirected to browser to log in in AWS and cache your credentials. Return to terminal and you‚Äôll get message like this:1234Switch to &apos;Login with Amazon&apos; page...Tokens fetched and recorded in ask-cli config.Vendor ID set as XXXXXXXXXXXXXXXProfile [default] initialized successfully. IMPORTANT ‚ùóÔ∏è‚ùóÔ∏è If you are using Multi-Factor Authentication and/or Session Token make sure you update the variables in ~/.aws/credentials at [default] profile section. Do in that way, because, ask-cli will [default] profile from this file to authenticate to AWS. See more info here Create a new project using an existing template We can use ask new command to create a new project using an existing template from the official templates.1ask new --template &quot;City Guide&quot; --skill-name &quot;my-skill-name&quot; After creating the new skill, we get this message 1&apos;my-skill-name&apos; skill package has been created based on the chosen template Note: ask new command is not going to upload anything to the server, that is just create the project folder locally. A new folder with the name my-skill.name will be created under your current path. Create .gitignore (optional) If you are using git I suggest you to create .gitignore file in your root folder ignoring node_modules folder at root or sub-folders .gitignore1node_modules Deploy the skill In my case, deployment is quite special case as I already have created AWS Lambda function. Normally if you do ask deploy it deploys everything, including skill, model and lambda. In our case I‚Äôm going to deploy skill and models using ask deploy and then lambda separately. Deploy Skill + Model first time Firstly, we have to publish skill + interaction model and after we will change the configuration to include the already created lambda function. In order to remove the lambda function configuration for doing the first deployment we must configure .ask/config and skill.json files in that specific way: .ask/config: skill_id should be blank. A new skill_id will be created and added automatically in config file. apis.custom should be blank. So, we want lambda function be empty on first time deployment.123456789101112131415&#123; &quot;deploy_settings&quot;: &#123; &quot;default&quot;: &#123; &quot;skill_id&quot;: &quot;&quot;, &quot;was_cloned&quot;: false, &quot;merge&quot;: &#123; &quot;manifest&quot;: &#123; &quot;apis&quot;: &#123; &quot;custom&quot;: &#123;&#125; &#125; &#125; &#125; &#125; &#125;&#125; skill.json: apis.custom should be blank. So, we want lambda function be empty on first time deployment.12345678910111213141516&#123; &quot;manifest&quot;: &#123; &quot;publishingInformation&quot;: &#123; &quot;locales&quot;: &#123; &quot;en-GB&quot;: &#123; ... &#125; &#125;, ... &#125;, &quot;apis&quot;: &#123; &quot;custom&quot;: &#123;&#125; &#125;, &quot;manifestVersion&quot;: &quot;1.0&quot; &#125;&#125; After removing the lambda configuration, we can deploy the skill 1ask deploy Then, we‚Äôll receive this message from console: 123456-------------------- Create Skill Project --------------------Profile for the deployment: [default]Skill Id: amzn1.ask.skill.1328031f-7fe2-4b78-a090-XXXXXXXXXXXXSkill deployment finished.Model deployment finished.[Info]: No lambda functions need to be deployed. We can also check the skill_id property has been filled now in the .ask/config file: 1&quot;skill_id&quot;: &quot;amzn1.ask.skill.1328031f-7fe2-4b78-a090-XXXXXXXXXXXX&quot; At this point we also can check our skill has been deployed in Alexa Skills Kit Developer Console: Update lambda function bits on deployment metadata Now our skill has been created for the first time, then we can update the lambda function metadata in order to be deployed / updated later on when we use ask deploy again. In order to be able to deploy our skill with a different lambda function configuration we have to do two things: 2.1. Update skill_id in our lambda function trigger (Alexa Skill Kit). We can add and configure a trigger for a lambda function through AWS Console Note: Remember to click Add and Save too. 2.2. Re-deploy the skill updating lambda function source files folder and endpoint ARN Before re-deploy, we have to configure our lambda source files and also our lambda function name in AWS (ARN). We do that, by changing these two files in our project: skill.json: apis.custom we have to add a new endpoint object with a sourceDir property set to ‚Äúlambda/custom‚Äù, which is the path or our source files.1234567&quot;apis&quot;: &#123; &quot;custom&quot;: &#123; &quot;endpoint&quot;: &#123; &quot;sourceDir&quot;: &quot;lambda/custom&quot; &#125; &#125;&#125; .ask/config: apis.custom we have to add a new endpoint object with a uri property set to the name of our function (ARN).1234567&quot;apis&quot;: &#123; &quot;custom&quot;: &#123; &quot;endpoint&quot;: &#123; &quot;uri&quot;: &quot;arn:aws:lambda:eu-west-1:XXXXXXXXXXXX:function:lambdaName&quot; &#125; &#125;&#125; Then we can re-deploy: 1ask deploy You‚Äôll get this result: 1234567-------------------- Update Skill Project --------------------Skill Id: amzn1.ask.skill.1328031f-7fe2-4b78-a090-XXXXXXXXXXXXSkill deployment finished.Model deployment finished.Lambda deployment finished.Your skill is now deployed and enabled in the development stage.Try invoking the skill by saying ‚ÄúAlexa, open &#123;your_skill_invocation_name&#125;‚Äù or simulate an invocation via the `ask simulate` command. üöÄ üöÄ We have everything deployed üöÄ üöÄ Check the skill deployment was successfulWe can go to our Alexa Skills Kit Developer Console and click in our skill to see the detailed configuration. Then we can see everything is green in our ‚ÄúSkill Check List‚Äù: Test the skillAlso you can click on ‚ÄúTest‚Äù tab to test the skill. More ResourcesMore information about AWS Lambda env variables: https://docs.aws.amazon.com/lambda/latest/dg/current-supported-versions.html https://docs.aws.amazon.com/cli/latest/userguide/cli-environment.html https://forums.developer.amazon.com/questions/119998/set-lambda-runtimehandler-from-ask-deploy.html","pubDate":"Wed, 25 Apr 2018 07:00:24 GMT","guid":"https://blog.josequinto.com/2018/04/25/deploy-alexa-skill-using-already-created-lambda-function-and-role/","category":"How-To"},{"title":"Building my first Voice App for Amazon Echo (Alexa) Skill with NodeJS","link":"https://blog.josequinto.com/2018/04/23/building-voice-app-custom-alexa-skill-node-js/","description":"IntroductionFirst time I heard about Alexa / Echo üëÄ I loved the idea of having an assistant at home capable of playing music on demand, setting up alarms, giving the flash briefings, an so on.Currently, I‚Äôm building a Voice App using Amazon Echo - Alexa and Google Home üíª. That‚Äôs quite interesting challenge given the fact I‚Äôll be using AWS Lambda function and NodeJS as part of the technology stack. This post is intended to gather all my notes meanwhile I was learning and developing my first Voice App using Alexa Skills Kit. I thought it will be important to share this notes as I felt quite lost at the beginning. Note: I‚Äôll be using JavaScript and NodeJS as a development language and engine for my first Alexa Custom Skill. ConceptsWhen I started looking at the documentation üë®‚Äçüíª, I realized there is a completely new vocabulary and concepts. Let‚Äôs see some of the concepts I learnt: Alexa Skill: It is the basic unit of Voice App for Amazon Echo. So as a developer you can develop and publish a new Alexa Skill and as a end user you can search and install that Alexa Skill in your Amazon Echo. There are different kind of skills. Alexa SKills Kit: The Alexa Skills Kit (ASK) is a collection of self-service APIs, tools, documentation, and code samples that makes it fast and easy for you to add skills to Alexa. Alexa Skills Kit Developer Console: It is the online tool to Build, Manage, Test, Launch, Measure and Delete your Alexa Skills. You have to sign up for an Amazon Development Account in order to use it. Skill Id: Unique identifier for your skill. It is very useful when you need to connect your AWS Lambda function with your Skill through a trigger called Alexa Skill Kit. Invocation Name: It‚Äôs the name used by Alexa to open our app or skill. Intent: It‚Äôs a way to group common and reusable actions or intentions. An intent represents an action that fulfills a user‚Äôs spoken request. Intents can optionally have arguments called slots. Also is the basic unit of development for a custom Alexa Skill. Each custom Intent have to define the intent schema plus the code to handle that intent. There are some standard built-in intents. Utterances: A set of likely spoken phrases mapped to the intents. They are different ways to invoke or activate an intent. For example for CancelIntent, we have these utterances: cancel, never mind and forget it. Slots: A representative list of possible values for a slot. For example, the intent I want to travel to {citySlot} could have different values for citySlot like: Paris, Amsterdam, Brussels, and so on. Interaction Model: It is where you implement the logic for the skill, and you also define the voice interface through which users interact with the skill. To define the voice interface, you map users‚Äô spoken input to the intents your cloud-based service can handle. It includes also Invocation Name, Intents and Slot Types. EndPoint: It is the place where we configure the ARN for AWS Lambda function that will handle the requests for the user interactions or intents. For example, we can define some custom Intents in Alexa Skills Kit Developer Console and then implement the logic for that intent in AWS Lambda function. Eventually we have to connect both using the EnpPoint feature available in Alexa Skills Kit Developer Console. ASK SDK: The ASK SDK v2 for Node.js makes it easier for you to build highly engaging skills. The most useful methods it provides to handle skill responses are: addRequestHandlers, addRequestInterceptors, addErrorHandlers and lambda. AWS SDK: It is important to notice the difference between ASK SDK (useful to handle skill requests) and AWS SDK (useful to connect our skill with AWS Services). You can see more samples in the Alexa Skill Building Cookbook. ASK CLI: Alexa Skills Kit Command Line Interface (ASK CLI) could be installed using npm. It exposes some high-level commmands like init, new, deploy, clone, validate, and simulate that we can use in the command line. ASK CLI Command Reference. AWS CLI: Amazon Web Services Command Line Interface could be useful in some Alexa advanced projects that requires other AWS Services to be created. RequirementsOnce we are more familiar with the main concepts. Before starting to develop a custom skill, there are some requirements: Register for an Amazon Developer Account Register for an AWS Account Note you can request for promotional credits on AWS for Alexa As we will be using NodeJS: NodeJS NPM Install and Setup AWS CLI Install and Setup ASK CLI Project StructureOnce you have installed all the requirements on your laptop, let‚Äôs see what is the project structure or skeleton required for Alexa Skill. The project structure for Alexa Custom Skill is: 1234567891011121314skill.json // skill manifest to define metadata.ask |- config // config file for ask clilambda |- custom |- node_modules // must be uploaded to AWS Lambda function |- index.js // lambda function to define intent handlers |- package.json // npm dependenciesmodels // Define Interaction Model for each language. |- en-US.json |- en-GB.json Skill Manifest Schema Interaction Model Schema As a beginner in the Voice Apps world, it is important to note that this is the structure used to define our Skill metadata, our Interaction Model schema and our Handlers code. It is important to keep that structure like that because ask-cli will be using it in order to deploy to Alexa Skills Kit Developer Console and AWS Lambda. Further explanation about the files: skill.json: where skill metadata lives. For example, skill_id or Lambda function ARN. .ask/config: ask-cli will be required to automate the creation, publication and update for our interaction model or lambda function. This config file is where our ask cli config lives. Note that authentication config for lambda function could be handled separately. lambda/custom/*: where our intents handlers code lives. In order to test it we should deploy using ask-cli. We‚Äôll see how later in the post. Here is where we‚Äôll use ask sdk to develop handlers for our intents. models/*: where our interaction model json definition lives. Whatever we define here as JSON format, we should implement the handlers in the lambda function code. Starting templates / Code SamplesThere are some üöÄ official boilerplates / code samples üöÄ to start with. All of them are github projects, so you can clone and start playing with them. I recommend to go through one of them and get it deployed and tested via your test environment provided by Alexa Skills Kit Developer Console: Amazon Official Code Samples IMPORTANT: Recently (April 2018), it was published the V2 for Alexa Skill Kit SDK. So, make sure that the code you are using in your sample is pointing to V2 and not V1. ‚ùå V1 uses require(&quot;alexa-sdk&quot;) ‚úÖ V2 uses require(&quot;ask-sdk-core&quot;) ProblemsAs part of my up and running for my first Alexa Skill, I found some issues in the process, I documented some solutions: ‚ùó Ô∏èProblem 1 ‚ùóÔ∏èA valid interaction model is required to test your skill there was an internal server error Solution: Check in Alexa Skills Kit Developer Console your Skill builder checklist. Everyone of these 4 items should be check in green! ‚ùóÔ∏è Problem 2 ‚ùóÔ∏èError parsing the requested content. Please validate the enums in the request, which is a common cause for this exception Solution: https://forums.developer.amazon.com/questions/165659/error-parsing-the-requested-content-please-validat.html Other Resources Alexa SDK for NodeJS (V1), ASK SDK V1 npm Alexa SDK for NodeJS (V2), ASK SDK V2 npm Alexa Cookbook I hope these notes will help on clarify little bit the new concepts on Voice Apps. Stay tunned ü§ò as I‚Äôll be publishing more post about advanced scenarios and deployments for Alexa Custom Skills!","pubDate":"Mon, 23 Apr 2018 07:00:24 GMT","guid":"https://blog.josequinto.com/2018/04/23/building-voice-app-custom-alexa-skill-node-js/","category":"How-To"},{"title":"Changing my Blogging Platform to GitHub Pages","link":"https://blog.josequinto.com/2018/02/08/changing-my-blogging-platform-to-github-pages/","description":"Introduction ‚ÄúWhat‚Äôs dangerous is not to EVOLVE‚Äù - Jeff Bezos I second that quote of Jeff Bezos (CEO of Amazon). Changes are always good, there is always a lots of learnings when we get our hands dirty changing things and trying to improve something. I decided to move my Blog from WordPress to Node.js and Github Pages. That wasn‚Äôt quick decision, it took to me some weeks to investigate about different platforms, hosting types and technologies. I‚Äôll try to go through every important point because I‚Äôm pretty sure my conclusions can help anyone else in the same situation. HostingEveryday is a learning experience üë®üèª‚Äçüè´, I was told by a colleague I could use GitHub Pages as a free hosting as long as I used something called Static Site Generator. Then, I started to investigate, and it turned out to be amazing idea üí°. In a nutshell, GitHub Pages is able to provide free hosting for Static Pages (html, css, js, assets, ‚Ä¶) and our laptop is able to run on-demand server to provide those assets. The only thing we need is a tool to create all the static assets and that‚Äôs called Static Site Generator. To be honest, that makes a lot of sense to me, as we can save money by using our laptop as a on demand server every time we need to publish a new content. üëâ I chose GitHub Pages as a free hosting. Static Site Generator (Blogging Platform)That‚Äôs quite personal decision, as there is a bunch of Static Site Generators depending on the technology, language, template engine or framework we want to work with: Gatsby: Static Site Generator for React. Jekyll: Static Site Generator for Ruby supporting Liquid template engine and Markdown markup language. Hexo: Static Site Generator specific for Blogs developed using JavaScript and supporting SWIG, EJS, HAML and PUG template engines and Markdown markup language. There are more Static Site Generators, but my investigation ended here as I hadn‚Äôt unlimited time for that. What I DO like of Hexo is the fact it is quite specific for blogging and already have lots of themes and plugings. üëâ I chose Hexo as a Blogging Platform and PUG as a template language. But in the near future I‚Äôm going to migrate it to Gatsby using React. MarkdownI used to write my blog posts using Windows Live Writer because it handles really well copying and pasting from code editors, you could copy and paste images as well and its integration with WordPress was üëå. Currently, I use macOS at work and Windows at home, then I need a multi-device writing platform. Also, I do use markdown for writing documentation, commenting on github, and so on. There is a pluging for Hexo called hexo-migrator-wordpress which helped a lot during the migration of all my 66 blog post from WordPress to Markdown. Even tough, I had to review and adapt most of them by hand lastly üöú ‚Ä¶ üëâ I chose the combination between YAML (metadata) and Markdown as a markup language and VS Code as a IDE. HTTPS, HTTP/2, CDN, Cache, ProxyUsing GitHub Pages we can enable HTTPS for our custom domain, but I wouldn‚Äôt recommend to use that because you lose control over lots of things like CDN support, scalability, continuous deployment and so on. There are lots of tools or platforms to act as a proxy between your files and the user using your custom domain. Let‚Äôs name two included on my research: Now.sh Node.js Server for free. Great Console cli. Netlify Provides free SSL certificate (One-Click SSL) by Let‚Äôs Encrypt (self-renewing). Easy to connect with your Github Project and branch. Continuos Deployment. One of the best First Byte Time. Free for Personal Projects. It supports HTTP/2. üëâ I chose Netlify because it‚Äôs One-Click SSL, integration with Github and First Byte Time. New Web App and DesignI wanted a fresh and fully customizable web app and design üöÄüöÄüöÄ. I started with a Hexo theme called melody, but eventually I fully customized it in order to achieve all the the requirements above. Great Page Load Mobile First Responsive Offline First PWA Fixed Nav Menu Table of Contents for posts Back to top Cards Design for listings Comments (Disqus) Search (Algolia) Smooth scrolling Great About Page using Responsive Skill Card Image optimization (ImageOptim) Integration with Medium To see further details about the implementation that‚Äôs the Github Project: ‚≠êÔ∏èÔ∏èÔ∏èÔ∏èÔ∏è Jos√© Quinto Blog ‚≠êÔ∏èÔ∏èÔ∏èÔ∏èÔ∏è SEOCurrently, I‚Äôm having around 8.000 Page Views per month. It‚Äôs not a lot, but still it‚Äôs quite important to me not to loose those visits as a result of the migration. That‚Äôs more important if we are migrating content and domain too, but still we have to be careful and track everything after the migration to be sure SEO isn‚Äôt not affected negatively. That‚Äôs my SEO action list: Redirect all HTTP to HTTPS requests (that‚Äôs easy with Netlify) Create permalinks with the exact URL we had before Create 404 Page with Contact information (just in case) Track and monitor Google Analytics to see the impact Re-link your WebMaster Account Re-link your Google Analytics tag Add canonical meta tag If some URL have changed 301 Redirects. OLD ‚Äì301‚Äì&gt; NEW. If Change domain Submit a Change Domain in WebMaster Tools (Google, Bing, ‚Ä¶) Rewrite all hardcoded links from your site (if any) 301 Redirects Check links to your site (webmaster tool) and ask them to update your url Read more Update Backlinks Update all your Profiles Twitter Github Google Account Facebook LinkedId Forums Stackoverflow CodePen Update Github GISTS (the content) Publishing WorkflowI usually like to copy things that works well and there is no shame on doing that as long as you give proper credits. I got lots of my ideas from my friend Jos√© Manuel Perez and I‚Äôd like to recommend üîù his post üîù about Platform and Publishing Workflow","pubDate":"Thu, 08 Feb 2018 08:00:24 GMT","guid":"https://blog.josequinto.com/2018/02/08/changing-my-blogging-platform-to-github-pages/","category":"How-To"},{"title":"How to configure webpack for replacing your API module path by a Mock API Path","link":"https://blog.josequinto.com/2017/07/10/how-to-configure-webpack-for-replacing-your-api-module-path-with-a-mock-api/","description":"IntroductionI‚Äôve been working with React, webpack and TypeScript for almost two years, and it still surprise me when I look for a really custom solution and I end up with a generic solution which will solve my specific problem in a generic way. Today, I am going to write about how to configure webpack to provide different Module path depending on our configuration. In my scenario I have two different APIs one is the ‚ÄúREAL‚Äù one and other is the ‚ÄúMOCK‚Äù. And I‚Äôd like to use the mocked API for development configuration and the real one for Production. Let‚Äôs say we have a function to get all terms of a given taxonomy term set (in SharePoint Online) in order to display them in a React application. We are going to describe how to create the real API, Mock API, how to configure webpack to do the ‚Äúgeneric replacement‚Äù and how to use the API. Real API Mock API How to configure webpackMy recommendation is using a NormalModuleReplacementPlugin in our development configuration for webpack: Usage","pubDate":"Mon, 10 Jul 2017 15:19:17 GMT","guid":"https://blog.josequinto.com/2017/07/10/how-to-configure-webpack-for-replacing-your-api-module-path-with-a-mock-api/","category":"How-To,Architecture-Concept"},{"title":"Dynamic Import Expressions and webpack 2 Code Splitting integration with TypeScript 2.4","link":"https://blog.josequinto.com/2017/06/29/dynamic-import-expressions-and-webpack-code-splitting-integration-with-typescript-2-4/","description":"OverviewTwo days ago (27/06/2017), was released TypeScript 2.4.0. Really good news to see that now dynamic import expressions are supported!. Dynamic import expressions are a new feature and part of ECMAScript that allows users to asynchronously request a module at any arbitrary point in your program. TC39 JavaScript committee has it‚Äôs own proposal which is in stage 3, and it‚Äôs called import() proposal for JavaScript. From other side, webpack bundler has a feature called Code Splitting ‚Äì Async: Code Splitting ‚Äì Async allows you to split your bundle into chunks which can be downloaded asynchronously at a later time. For instance, this allows to serve a minimal bootstrap bundle first and to asynchronously load additional features later. At first glance, I did a strict relationship between these two features. I mean, it‚Äôs natural to think (if we are using webpack in our dev workflow) that by using TypeScript 2.4 dynamic import expressions, will automatically produce bundle chunks and automatically code-split you JS final bundle. BUT, that is not as easy as it seems, because it depends on the tsconfig.json configuration we are working with. The thing is that webpack code splitting supports two similar techniques to achieve this goal: using import() (preferred, ECMAScript proposal) and require.ensure() (legacy, webpack specific). And what that means is the expected TypeScript output is leave the import() statement as it is instead of transpile it to anything else. Let‚Äôs see and example to figure out how to configure webpack + TypeScript 2.4. In the following code I want to lazy load the library moment but I am interested on code splitting as well, which means, having moment library in a separate chunk of JS (javascript file) and that will be loaded only when required. Unexpected configuration for Code Splitting with webpack Output: Note how the JS output from TypeScript 2.4.0 is resolving directly with a Promise.resolve() instead of using import(). Then, even if that solution will work in terms of functionality, but not in terms of code splitting, because that is NOT the input expected by webpack. Expected TypeScript configuration for Code Splitting with webpack Output: However, this output is the ideal for webpack Code Splitting, and we don‚Äôt have to worry about the other ‚Äúnormal‚Äù imports which are transpiled using esnext, because webpack knows how to handle all of them (imports and exports). Conclusions Using ‚Äúmodule‚Äù: ‚Äúesnext‚Äù TypeScript produces the mimic impot() statement to be input for Webpack Code Splitting. Currently (TypeScript 2.4.0) There is a bug using ‚Äúmodule‚Äù: ‚Äúesnext‚Äù. Some external libraries like moment are not recognized by TypeScript if you don‚Äôt configure explicitly ‚ÄúmoduleResolution‚Äù : ‚Äúnode‚Äù. PLEASE, DON‚ÄôT FORGET TO INCLUDE IT IN YOUR TSCONFIG.JSON. You can see a working sample here: TypeScript 2.4, Dynamic Import Expressions with webpack Code Splitting Sample. Some TypeScript forums for reference: [Master] wip-dynamic import [Design Spec] ESNext import()","pubDate":"Thu, 29 Jun 2017 09:14:41 GMT","guid":"https://blog.josequinto.com/2017/06/29/dynamic-import-expressions-and-webpack-code-splitting-integration-with-typescript-2-4/","category":"New-Feature-Info,Github-Project"},{"title":"Part 5. How to consume our decorators, models and parsers from SPFx, the winning combination","link":"https://blog.josequinto.com/2017/06/28/how-to-consume-our-decorators-models-and-parsers-from-spfx-the-winning-combination/","description":"Post Series IndexThis is a blog post in the series about working with Custom Business Objects, Parsers and Decorators in PnP JS Core: Introduction to Why do we should use Custom Business Objects (Models) in PnP JS Core Creating select and expand TypeScript Property Decorators to be used in PnP JS Core Creating MyDocument and MyDocumentCollection models extending Item and Items PnP JS Core classes Create Custom Parser and Array Parser to unify select and property names How to consume our decorators, models and parsers from SPFx, the winning combination (this article) Github project! Please remember to ‚Äústar‚Äù if you liked it! IntroductionIn the previous posts of this series we explained why we should use Custom Business Objects in PnP JS Core and we implemented TypeScript decorators, Custom Business Object inheriting from Item and Items PnP JS Core classes, and custom Parsers. In this article, we will see how to use all of them together in order to get the max benefit from querying PnP JS Core from SharePoint Framework Web Part. SPFx Web Part sampleAll code samples we are going to see here, actually, are implemented in this Github project: spfx-react-sp-pnp-js-property-decorators. There are some requisites to have in order to run this webpart sample. 1. Create a list called PnPJSSample with four columns (ID, Title, Category and Quantity). 2. Upload some documents in the Documents library. In the following code sample, we can see different ways to consume and query against PnP JS Core using different combinations of decorators, models and parsers: Browser Console ScreenshotsWe can see after looking into the different code samples, how they actually work, and what is the result from the browser console: ConclusionIn my opinion, if we are going to create custom classes in our TypeScript projects for consuming SharePoint lists by using PnP JS Core, the ideal is being integrated with it. And by using decorators will do our life easier and our code more maintainable. There are multiple ways to create and consume custom objects and parsers, but this post is intended to show the differences and give you an overview in order to decide what to use. From my point of view, the ideal way to consume is: Console Result: And the reason is because: We can use @select and @expand decorators efficiently We can continue using method chain after as(MyDocumentCollection). For example skip(1) JavaScript objects returned in the Array are named MyDocument (better for debugging) We can use intellisense in VS Code with our custom model properties (Size, Title and Name).","pubDate":"Wed, 28 Jun 2017 10:33:42 GMT","guid":"https://blog.josequinto.com/2017/06/28/how-to-consume-our-decorators-models-and-parsers-from-spfx-the-winning-combination/","category":"Series,PnPJsCore"},{"title":"Part 4. Create Custom Parser and Array Parser to generate query and property names in PnP JS Core","link":"https://blog.josequinto.com/2017/06/28/create-custom-parser-and-array-parser-to-generate-query-and-property-names-in-pnp-js-core/","description":"Post Series IndexThis is a blog post in the series about working with Custom Business Objects, Parsers and Decorators in PnP JS Core: Introduction to Why do we should use Custom Business Objects (Models) in PnP JS Core Creating select and expand TypeScript Property Decorators to be used in PnP JS Core Creating MyDocument and MyDocumentCollection models extending Item and Items PnP JS Core classes Create Custom Parser and Array Parser to unify select and property names (this article) How to consume our decorators, models and parsers from SPFx, the winning combination Github project! Please remember to ‚Äústar‚Äù if you liked it! IntroductionIn the previous posts of this series we explained why we should use Custom Business Objects in PnP JS Core and we implemented TypeScript decorators and Custom Business Object inheriting from Item and Items PnP JS Core classes to help us to have more generic and maintainable code . In this article, we will see how to solve some specific issue to unify query and business object properties by creating custom Parser and Array Parser in PnP JS Core. What is a custom parsers in PnP JS Core?PnP JS Core makes use of parser classes to handle converting the response object returned by fetch into the result. The default parser implemented, ODataDefaultParser is used for all requests by default (except a few special cases). But, we can create a custom parser by extending ODataParserBase class. What is the difference between Parsers and Array Parsers?Parsers process returned single item. Array Parsers process returned Item Collections as an array. Parser and Array Parser implementationHere we have both implementations for SelectDecoratorArrayParser and SelectDecoratorParser which is using the decorator metadata in order to combine the results and provide our real business object. For example, in the previous post, we did map @select(‚ÄúFile/Length‚Äù) with the property called Size, then the parser will be actually provide the information in the proper Size property. How to use Custom Parsers?Here is an example of consuming information from a SP library by using custom objects and custom parsers. Sample 1. Query one single document using MyDocument custom object and SelectDecoratorParsers (single) Sample 2. Query multiple documents using MyDocument and MyDocumentCollection custom object classes and SelectDecoratorsArrayParser (returning just the Custom Object properties): Sample 3. Query multiple documents using MyDocument and MyDocumentCollection custom object classes and SelectDecoratorsArrayParser (returning the full PnP JS Core object): ConclusionThe ideal scenario is using the sample 3 because will allows us continue with the pnp js core method chain if needed.","pubDate":"Wed, 28 Jun 2017 09:40:26 GMT","guid":"https://blog.josequinto.com/2017/06/28/create-custom-parser-and-array-parser-to-generate-query-and-property-names-in-pnp-js-core/","category":"Series,PnPJsCore"},{"title":"npm install fails on Windows 10: ENOENT 4058 operation not permitted, rename","link":"https://blog.josequinto.com/2017/06/23/npm-install-fails-on-windows-10-enoent-4058-operation-not-permitted-rename/","description":"IntroductionRecently, I‚Äôve faced different issues when tryng npm install in some of the project I‚Äôve been working with. Environment Windows 10 (OS Build 15063.413) Node version: 8.1.2 NPM version: 5.0.3 Every time I tried npm install or install a isolated package I got errors like this one: Problem1234567_npm ERR! path \\GitHub\\react-typescript-webpack2-cssModules-postCSS\\node_modules\\ts-loadernpm ERR! code ENOENTnpm ERR! errno -4058npm ERR! syscall renamenpm ERR! enoent ENOENT: no such file or directory, rename 'D:\\GitHub\\react-typescript-webpack2-cssModules-postCSS\\node_modules\\ts-loader' -&gt; '\\GitHub\\react-typescript-webpack2-cssModules-postCSS\\node_modules\\.ts-loader.DELETE'npm ERR! enoent This is related to npm not being able to find a file._ SolutionThe solution for my specific scenario was: 1. Close VS Code 2. Remove package-lock.json, node_modules and npm cache 1234# Windowsdel package-lock.jsonrd /s /q node_modulesnpm cache clear --force 1234# macOsrm package-lock.jsonrm -rf node_modules/npm cache clear --force 3. Retry npm i Read more‚Ä¶If you still have this problem, read this: https://github.com/npm/npm/issues/10826","pubDate":"Fri, 23 Jun 2017 14:08:53 GMT","guid":"https://blog.josequinto.com/2017/06/23/npm-install-fails-on-windows-10-enoent-4058-operation-not-permitted-rename/","category":"Problem-Solution"},{"title":"Part 3. Creating MyDocument and MyDocumentCollection models extending Item and Items PnP JS Core classes","link":"https://blog.josequinto.com/2017/06/15/creating-mydocument-and-mydocumentcollection-models-extending-item-and-items-pnp-js-core-classes/","description":"Post Series IndexThis is a blog post in the series about working with Custom Business Objects, Parsers and Decorators in PnP JS Core: Introduction to Why do we should use Custom Business Objects (Models) in PnP JS Core Creating select and expand TypeScript Property Decorators to be used in PnP JS Core Creating MyDocument and MyDocumentCollection models extending Item and Items PnP JS Core classes (this article) How to consume our decorators, models and parsers from SPFx, the winning combination How to consume our decorators, models and parsers from SPFx, the winning combination Github project! Please remember to ‚Äústar‚Äù if you liked it! IntroductionIn the previous posts of this series we explained why we should use Custom Business Objects in PnP JS Core and we implemented TypeScript decorators to help us to have more generic and maintainable code. In this article, we will see how to implement Custom Business Objects inheriting from Item and Items PnP JS Core generic classes and using the previously created TypeScript decorators internally. What is the difference between Item and Items PnP JS Core classes?Let‚Äôs do the comparison with Client Object Model, Item is ListItem and Items is ListItemCollection. PnP Core JS expose these two different classes Item and Items with different methods within them. For example, you can see different Item Methods and Items methods. Imagine we are trying to get specific Item or Document from SharePoint using PnP Core JS then we will use this code (with no custom objects): And similarly, to get Items or Document Collection we will use: Here the result: Now, we already know the difference between Item and Items from PnP JS Core. Let‚Äôs implement our two custom classes inheriting both of them and combining TypeScript decorators. Custom classes implementation inheriting from Item and ItemsWe are going to create two new classes called ‚ÄúMyDocument‚Äú and ‚ÄúMyDocumentCollection‚Äú. MyDocument MyDocumentCollection Note you can see the full code in this github project. Ideally we will create a folder in our solution to create all our models: How to use MyDocument Custom Business Object from our PnP JS Core codeWe can easily use the class with the following code: How to consume MyDocument: How to use MyDocumentCollection: ConclusionWe can notice how both custom objects and decorators work well because queries to SP only brings the right data ‚ÄúTitle, FileLeafRef and File/Length‚Äú, but still we need a parser in order to correctly do the mapping between the query properties into our TypeScript objects ‚ÄúTitle, Name and Size‚Äú properties. In the next post we are going to implement a custom Parser and Array Parser to solve this specific issue.","pubDate":"Thu, 15 Jun 2017 22:22:47 GMT","guid":"https://blog.josequinto.com/2017/06/15/creating-mydocument-and-mydocumentcollection-models-extending-item-and-items-pnp-js-core-classes/","category":"Series,PnPJsCore"},{"title":"Part 2. Creating select and expand TypeScript Property Decorators to be used in PnP JS Core","link":"https://blog.josequinto.com/2017/05/29/creating-select-and-expand-typescript-property-decorators-to-be-used-in-pnp-js-core/","description":"Post Series IndexThis is a blog post in the series about working with Custom Business Objects, Parsers and Decorators in PnP JS Core: Introduction to Why do we should use Custom Business Objects (Models) in PnP JS Core Creating select and expand TypeScript Property Decorators to be used in PnP JS Core (this article) Creating MyDocument and MyDocumentCollection models extending Item and Items PnP JS Core classes How to consume our decorators, models and parsers from SPFx, the winning combination How to consume our decorators, models and parsers from SPFx, the winning combination Github project! Please remember to ‚Äústar‚Äù if you liked it! IntroductionIn the previous post of this series we explained why we should use Custom Business Objects in PnP JS Core and at the end of the article we summarized some improvements to work with Business Objects in a more generic and usable way. In order to achieve it, we propose the usage of TypeScript Decorators in combination with Custom Business Objects and Custom Generic Parsers. In this article, we are going to see what are TypeScript property decorators and how to implement them for this specific scenario. What are TypeScript Property Decorators?Generally speaking, decorators are special bindings to easily provide extra functionality for classes, methods, accessors, properties and parameters. TC39 members are working on the definition of a standard for ECMAScript Decorators due to the success of TypeScript experimental decorators‚Äô implementation and to the good acceptance **of them on Angular, Aurelia and Ember frameworks. In Typescript, A Decorator is a special kind of declaration that can be attached to a class declaration, method, accessor, property, or parameter. There are the four different decorator declarations available (by May 2017 - TypeScript 2.3.3): In this post, we will only use ‚Äúproperty decorators‚Äú. For example, see how the class MyDocument could have a select decorator to query FileRefLeaf field and do the mapping with Name property in our class: How could Decorators help to Custom Business Objects in PnP JS CoreAs we described in the previous post of this series, we can define a Custom Business Object with this code: You can see in the code how we create and maintain 4 different properties in the class definition, and separately, we also maintain an array of names for every property we want to query against the server using SP Rest API with select parameter. The idea of decorators in this scenario is achieve something like: Property Decorators‚Äô implementation for select and expand in PnP JS CoreOnce we have more context about what is a decorator intended for, let¬¥s see how to implement property decorators and property decorator factory. Basically, a decorator factory is a function that returns a function of type PropertyDecorator. Have a look into the TypeScript type definitions for decorators and note how all of them are different, for example, PropertyDecorator type have two parameters target (the class or instance in which the property is) and propertyKey (the name of the property). Also note how PropertyDecorator in a way contrary to MethodDecorator returns nothing, which means that in order to add extra functionality we can¬¥t return a modified property as a result of the decorator function, we should modify the target object itself instead. Now, we are going to describe how to implement two decorators for select and expand functionalities. The idea of these decorator is to make an annotation on the class and store in the target object two lists of the properties tagged with @select and @expand in order to be used later at query time. Important Notes @select decorator implementation has queryName as optional, so if there isn¬¥t queryName, we are getting the property name itself to be used in the query. Decorator factories are used because we need to provide custom queryName for the query if needed. We extract the functionality setMetadata as a separate function as it will be used on both decorators. I considered to use reflect-metadata API to set metadata, but isn‚Äôt needed on this scenario as we are storing the metadata in the actual object (target) and we don¬¥t need extra overload. If you¬¥d like see the real implementation of decorators: take a look to this file which have all the decorators implements for PnP JS Core example. Let¬¥s see now how this implementation is working on a real SPFx webpart and what is its runtime behavior: In the code we have a PnP JS Core Custom Object which inherits from Item and because of that is provided with some already implemented methods get(), getAs(), constructor, and so on, that we can override to change some behavior (we will see in the next post how to override get() method to actually use the provided decorators. Apart from that, our custom class also defines two properties called Title and Name and they both are using @select decorator to set this property as queriable via ‚Äúselect‚Äù parameter of the REST API. The first one will use ‚ÄúTitle‚Äù in the query, and the second one will use ‚ÄúFileLeafRef‚Äù, which means a query like that: /_api/web/lists/getByTitle(‚ÄòPnPJSSample‚Äô)/items(1)?$select=Title,FileLeafRef How the metadata is stored using decorators?Let¬¥s see this example at runtime: Property 1: Title Property 2: FileLeafRef: We can see how we use the target object to store the information using select property. Ideally we should use ES Symbol when they have more browser support. When is decorator code executed?Code for property decorators is executed when JavaScript engine read (import) our custom class, just at the beginning. Imagine we define our custom class MyDocument as external module and we import this module from our webpart component .tsx file. Then, the code is executed just in the import evaluation, as we can see in the following picture: # Description 1 Import our Custom Object class from our tsx component. 2 and 3 When JavaScript engine evaluate the import is when it will evaluate and execute all decorator functions 4 After evaluating all decorator functions, the array of selected properties is already generated and stored on MyDocument, then it returns to the execution of out component, just after the import instruction. 5 In the get method of our PnP JS Core Custom Object we already have our selectors evaluated and we have the information we need stored in MyDocument object as own property","pubDate":"Mon, 29 May 2017 13:19:50 GMT","guid":"https://blog.josequinto.com/2017/05/29/creating-select-and-expand-typescript-property-decorators-to-be-used-in-pnp-js-core/","category":"Series,PnPJsCore"},{"title":"Part 1. Why Should we use Custom Business Objects (Models) in PnP JS Core","link":"https://blog.josequinto.com/2017/05/19/why-do-we-should-use-custom-business-objects-models-in-pnp-js-core/","description":"Post Series IndexThis is a blog post in the series about working with Custom Business Objects, Parsers and Decorators in PnP JS Core: Introduction to Why do we should use Custom Business Objects (Models) in PnP JS Core (this article) Creating select and expand TypeScript Property Decorators to be used in PnP JS Core Creating MyDocument and MyDocumentCollection models extending Item and Items PnP JS Core classes How to consume our decorators, models and parsers from SPFx, the winning combination How to consume our decorators, models and parsers from SPFx, the winning combination Github project! Please remember to ‚Äústar‚Äù if you liked it! IntroductionThe PnP JS Core library was created to help developers to simplify common operations within SharePoint and the SharePoint Framework by providing a fluent API (wrapper) for SharePoint REST API. A few months ago I started to put my hands on PnP JS Core and I created a new react sample showing the use of it with async / await. By then I noticed how easy accessing to REST API services is by using this library, which even provides an API to cache data in the browser and do query batching. Then I decided to give it a go for using in a real world application, and then the fun began ;) and I started to look into more advanced concepts like Extending with Custom Business Objects and create Response Parsers. I especially DO like TypeScript to develop modern JS Applications and webparts. Moreover, when developing an application we often have defined entities and models representing the data we are manipulating. More in particular in client side development and using TypeScript as a OO Language we like to have our data as a Typed object instead of using any :) The PnP JS Core library provides a base for us to build our custom objects at different levels: Item import { Item } from ‚Äúsp-pnp-js‚Äù; Items (ItemCollection) import { Items } from ‚Äúsp-pnp-js‚Äù; For the full list see this PnP JS Core github folder. Why do we should use custom business objects in PnP JS Core?Let‚Äôs get to the point, quick answer is we don‚Äôt strictly need create custom business objects in order to do queries with PnP JS Core library, but if we are using the combination of TypeScript, React and PnP JS Core it will be obvious the benefits it could give to our full development team. However I would try to show you these benefits by providing two examples solving the same requirement (Get Specific Item from a Custom SharePoint List). Let‚Äôs assume we have the following list with four columns: We are going to use PnP JS Core to query the first item of that list but we will do in two ways: PnP JS Core WITHOUT custom model PnP JS Core WITH custom model PnP JS Core WITHOUT custom modelLet‚Äôs assume we already have an environment in which use PnP JS Core, if not you can use this project as a reference to start with PnP JS Core, TypeScript, React and SPFx. Then let‚Äôs use the following code to query do the query: Here you see an image as well, in order to illustrate TypeScript compiling errors and intellisense: In the sample, we can see clearly the benefits of type checking and intellisense on TypeScript. Initially we are using any as a Type for plainItemAsAny object and we aren‚Äôt receiving help from the TypeScript compiler. In the following lines, we are using the ‚Äúas‚Äù operator inside .tsx file (introduced in TypeScript 1.6). We are using it as a workaround to get type checking and intellisense,&nbsp; which help avoiding some typical developer mistakes like accessing title property instead of Title with capital letter. Actually, ‚Äúas‚Äù operator inside .tsx file is the default way to cast objects (removing any ambiguity between JSX expressions and the TypeScript prefix cast operator). The next image shows the console output for the previous code: But, thinking little bit more about this scenario, we defined the type dynamically and only for this part of our code, which can‚Äôt be reused elsewhere. In addition, imagine we need to add some other properties in the model and we should to edit several parts of our code including the Types, .select(..) method and so on. Then building our own Custom Business Objects (Models) starts making lot of sense, isn‚Äôt it ? PnP JS Core with custom objectsWe‚Äôve seen how TypeScript helps building good quality code using Types in general, and specifically for PnP JS Core. Now, we are going to show how to create our custom Model (MyItem) extending Item class already provided by PnP JS Core. Create the custom object (model) in a separate .ts file: Note that we initially import Item and ODataEntity. Item is needed to be able to overrides get() method and ODataEntity will be used to provide the default PnP JS Core parser. How to use this custom object: Note that&nbsp; we import our custom model (MyItem) at the beginning and we create our constant with the specific type ‚ÄúMyItem‚Äù. Take a look to the code and comments to see the benefits. Let‚Äôs highlight how the TypeScript compiler warns us if we try do get some properties wrongly: Let‚Äôs remarks how can we use intellisense too: The following image shows the console output for this code: Note how the real and final JavaScript object has more properties which inherits directly from Item class plus the own MyItem properties. ConclusionWe have shown how using our custom business object in PnP JS Core DO help on: Type checking Intellisense Improve performance by overriding select parameter Compatibility with chaining methods Having only one source of the truth of out model (improve maintenance) However, there are other aspects could be improved like: Support for complex expand and select for sub-objects like .select(‚ÄúFile/Length‚Äù). That could be done using TypeScript decorators combined with a small function in our model class. Isolate query and property names. We could want FileLeafRef property to be called Name in our model. That could be done using generic custom parsers to be used across all our models. Avoid duplicate property definition in our code. For example, we have in our custom model 4 properties defined but we DO repeat the names in the static variable called Fields, which is used to override the select parameter afterwards. We could avoid defining properties twice by using TypeScript decorators. We have examples to override single Item, but not for Item Collections, and there are some features to consider when we implement custom collections. We can extends Items class instead Item to achieve it! We are going to cover each of these improvements in the following posts of this series.","pubDate":"Fri, 19 May 2017 16:59:12 GMT","guid":"https://blog.josequinto.com/2017/05/19/why-do-we-should-use-custom-business-objects-models-in-pnp-js-core/","category":"Series,PnPJsCore"},{"title":"How to integrate PnP JS Core and SharePoint Framework logging systems","link":"https://blog.josequinto.com/2017/04/30/how-to-integrate-pnp-js-core-and-sharepoint-framework-logging-systems/","description":"IntroductionRecently, I was playing with PnP JS Core library, which basically is a wrapper of SharePoint Rest API that allows us to easily use it. I discovered some cool Features in this library which I really like them! For example, we are able to easily cache queries, do batches, and the library has a good Logging system, which actually is the main topic of this post. Integrate LoggingSo, this post is intended to show how to integrate two different logging systems when developing a SharePoint Framework web part. Let¬¥s share few documentation links to be aware which logging systems I am talking about: SPFx Log class SPFx. Working with the Logging API PnP JS Logging implementation PnP JS. Working With: Logging React component logging with TypeScript After creating our SPFx web part, and installing sp-pnp-js, then let¬¥s import both logging systems classes: If we are using React to build our SPFx webpart, then we could use the following code in our React Component¬¥s constructor: We can see the comments as self-explanation how the integration have been done. So, basically we enable PnP JS Core Logging at Info level and we create a special FunctionListener to pass every message through SPFx Log system using the right method (verbose, info, warn or error). ExampleLet¬¥s see an example how it works, imagine we have this code in which we are querying a library selecting a non-existing column called ‚Äúassdafa‚Äù. PnP JS will provide an internal error giving more detail, we can see it on the following image: ConclusionNote that the source of the error is sp-loader instead of pnp-js, that is because we are using the FunctionListener to pass all the errors thru SPFx logger. I created a SPFx webpart sample to demonstrate this PnP JS functionality. Please check it out and any feedback is welcome!","pubDate":"Sun, 30 Apr 2017 22:10:24 GMT","guid":"https://blog.josequinto.com/2017/04/30/how-to-integrate-pnp-js-core-and-sharepoint-framework-logging-systems/","category":"How-To,Systems-Integration"},{"title":"All TypeScript 2.3 options for tsconfig.json (tsc --init)","link":"https://blog.josequinto.com/2017/04/28/all-typescript-tsconfig-json-options-using-typescript-init/","description":"IntroductionYesterday was released TypeScript 2.3 and one of the improvements was an easier startup with better help, richer init, and quicker strictness. It means that TypeScript‚Äôs --init output so that potential options are explicitly listed out in comments. As an example, tsconfig.json output will look something like the following: We can see how TypeScript team added a new flag called ‚Äústrict‚Äù which is activated by default when create ‚Äútsc ‚Äìinit‚Äù. --strict flag, which enables the following settings --noImplicitAny --strictNullChecks --noImplicitThis --alwaysStrict (which enforces JavaScript strict mode in all files) This --strict flag represents a set of flags that the TypeScript team believes will lead to the most optimal developer experience in using the language. You can see the full options in the official wiki page as well. Read more‚Ä¶You can read the full list of what‚Äôs new in TypeScript and read TypeScript‚Äôs Roadmap to see what‚Äôs coming in the future!","pubDate":"Fri, 28 Apr 2017 06:05:52 GMT","guid":"https://blog.josequinto.com/2017/04/28/all-typescript-tsconfig-json-options-using-typescript-init/","category":"Release-News"},{"title":"Boilerplate project for React, TypeScript, Webpack 2, postCSS, CSS-Modules and HMR","link":"https://blog.josequinto.com/2017/04/19/boilerplate-project-for-react-typescript-webpack-2-postcss-css-modules-and-hmr/","description":"IntroductionFew months ago, I was starting to use WebPack 2 and TypeScript on my React projects. To be fair with ES6 and Babel, I really enjoyed build applications with them and they are in my hearth and that is why I did other post as well, about how to build a new SPA with ES6, SCSS, React, Webpack and HMR. But, then I met TypeScript and to be honest at the beginning was so annoying to deal with typings‚Ä¶ but once you get used to it, that really rocks! Then I decided to create a boilerplate, starter template, (o whatever name you like more) to help me and other people in the community create the baseline project for our React applications written in TypeScript, using webpack 2 as a bundler system, using postcss combined with CSS Modules as a Style technique and having Hot Module Replacement as a powerful feature which allows us to see the changes reflected on the browsers immediately. You can see, download and use freely the boilerplate on this link: https://github.com/jquintozamora/react-typescript-webpack2-cssModules-postCSS. Folder Structure Lot‚Äôs of configurations are happening on this template, but I will recap the main features: Main Features React TypeScript (compiling directly to ES5) Using @types instead of TSD or typings folder Hot Module Replacement (React Hot Loader 3) Webpack 2 Webpack-dev-server Webpack configuration for HMR Webpack production configuration Split out css files using ExtractTextPlugin UglifyJsPlugin with options Use include in the loader instead of the exclude. More info More performance tips: here Webpack stats (bundle optimization helper) Generate stats.json file with profiler. Use [http://webpack.github.io/analyse/] to analyze it. webpack visualizer EditorConfig Styling General Styling (app/stylesheets): To include variables, generic CSS, normalize, reset, type selectors, ‚Ä¶ Methodology: ITCSS Tools: postCSS with import, nesting, custom properties and autoprefixer. Components Styling (app/src/components/‚Ä¶): To be the module‚Äôs CSS Techniques: CSS Modules + postCSS (import, nesting, custom properties and autoprefixer). Linting TypeScript: TSLint: general rules + react rules VS Code TSLint extension Styles Stylint: CSS rules Rules are on .stylelintrc.json All the rules Install VS Code extensions: stylelint stylefmt Shift + Alt + F (Format Code) Be sure you have these configurations on your .vscode/settings.json: ‚Äúcss.validate‚Äù: false, ‚Äústylelint.enable‚Äù: true StatsIf you run npm run stats you will generate a HTML file with the bundle stats. That is really helpful to see which npm packages are you including and the size of them. Useful demoAs part of the boilerplate, you will see a fancy demo app with two react components already created: Viewer Stateless component (no state) Use CSS-Modules ViewerItem Create 3 types of Item Card object (singleton, factory static) Use inline css using a function to assign CSS depending on the object Render Table layout compatible with emails [ Star it!If you like it, don‚Äôt forget to ‚òÖ on Github.","pubDate":"Wed, 19 Apr 2017 20:21:21 GMT","guid":"https://blog.josequinto.com/2017/04/19/boilerplate-project-for-react-typescript-webpack-2-postcss-css-modules-and-hmr/","category":"Github-Project,Boilerplate"},{"title":"How to Convert Array of Objects into Comma Separated String extracting only one property","link":"https://blog.josequinto.com/2017/03/17/how-to-convert-array-of-objects-into-comma-separated-string-extracting-only-one-property/","description":"I‚Äôd like to share a quick solution which always is really useful when you are handling complex object-type data structures in JavaScript / ES6 / TypeScript. That code will be useful when you want to extract some property values from an array of objects. I will share two versions one for EcmaScript 5 and other for EcmaScript 6:","pubDate":"Fri, 17 Mar 2017 16:59:20 GMT","guid":"https://blog.josequinto.com/2017/03/17/how-to-convert-array-of-objects-into-comma-separated-string-extracting-only-one-property/","category":"How-To,Quick-Note,Code-Reminder"},{"title":"TypeScript functions to get current domain, site collection and Site urls with window.location fallback","link":"https://blog.josequinto.com/2017/03/09/typescript-functions-to-get-current-domain-site-collection-and-site-urls-with-window-location-fallback/","description":"IntroductionRecently, I was working in a TypeScript project building a SPA application on top of Office 365 and I needed to get the current domain, current site collection and current site urls for using them in a different React components. The case is we always rely on _spPageContextInfo to get this information, but in fact, we can use window.location object to build the urls as a fallback in case we need to use this Utils library before SP.js loaded or in other context. CodeHere is my solution for that problem using TypeScript:","pubDate":"Thu, 09 Mar 2017 18:16:48 GMT","guid":"https://blog.josequinto.com/2017/03/09/typescript-functions-to-get-current-domain-site-collection-and-site-urls-with-window-location-fallback/","category":"How-To,Quick-Note,Code-Reminder"},{"title":"TypeScript functions to convert from Base64 to UTF8 and vice versa","link":"https://blog.josequinto.com/2017/03/08/typescript-functions-to-convert-from-base64-to-utf8-and-vice-versa/","description":"IntroductionFew days ago I wrote a post about uploading JSON object using JavaScript. There are some dependencies using this approach, which are b64EncodeUnicode and b64DecodeUnicode functions. I‚Äôd like to share how to write these functions using TypeScript:","pubDate":"Wed, 08 Mar 2017 18:19:58 GMT","guid":"https://blog.josequinto.com/2017/03/08/typescript-functions-to-convert-from-base64-to-utf8-and-vice-versa/","category":"How-To,Quick-Note,Code-Reminder"}]}